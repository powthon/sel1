from histogram import FrequencyGramimport sysimport os.pathimport pickleimport reimport jsonfrom syllapy import count as s#...................................................................ALPHABETS= "([A-Z a-z])"#....................................................................PREFIXES="(my great|fair|virtuous|valiant|vile|king|queen|count|princess|prince|lord|lady|sir|mistress|duke|worthy|my noble|noblelroyal|highness)[' ']" #"(In|un|dis|sub|lack|re|mis|de)" #[']#....................................................................#^^^^^^^^^^^^^# "(My Great|Fair|Virtuous|O valiant|Vile|King|Queen|#Count|Princess|Prince|Lord|Lady|Sir|Mistress|Duke|#worthy|Worthy|My noble|Noble)[.]" #Shakespearean#--------#"(Mr|St|Mrs|Ms|Dr)[.]"                          #Original#--------##....................................................................SUFFIXES="(Inc|Ltd|Jr|Sr|Co)"#"(ed|nt|ion|al|s|d|st|ll|er|th|est|t|ize|ful|less|ly|le|uous|ial|en|ve)"  #....................................................................#^^^^^^^^^^^^^^^^# "(III|II|IV|Cousin|Brother|villain|knave|ation|my'lord|lady|#the youngerldame|'d|'st|'ll|'er|'th)"   #Shakespearean#-----#"(Inc|Ltd|Jr|Sr|Co)"                              #Original#....................................................................STARTERS="(Methinks|Doth|Thine|Thy|Who\s|The|I|This|Not|Hast|Haveth|Makest|Tis|Twas|Thou|He\s|She\s|It\s|They\s|Their\s|Our\s|We\s|But\s|However\s|That\s|This\s|Wherever|Why|If)"   #....................................................................#^^^^^^^^^^^^^^^#"(O'|My|King|Queen|Come|Count|Princess|Prince|Lord|#Lady|Sir|Mistress|To|The|I|This|They|And|Not|He\s|#She\s|It\s|They\s|Their\s|Our\s|We\s|But\s|#However\s|That\s|This\s|Wherever|Why|If)"   #                   Shakespearean#----#"(Mr|Mrs|Ms|Dr|He\s|She\s|It\s|They\s|Their\s|Our\s|#We\s|But\s|However\s|That\s|This\s|Wherever)"#Original#....................................................................ACRONYMS = "([A-Z][.][A-Z][.](?:[A-Z][.])?)"WEBSITES = "[.](com|net|org|io|gov)"PUNCS = ",'.?!:;"  #....................................................................#....................................................................N_LIST="nouns.txt"V_LIST="verbs.txt"C_LIST="(and|or|but|if|while|although)"#MODEL_NAME =""#MODEL_NAME ="model.json"MODEL_NAME="model.pkl"TRAINING_DATA = "modified_shakespeare.txt"#TRAINING_DATA = "shakespeare.txt"#.....................................................................#^^^^^^^^^^^^^^^^^^#"/data/data/com.termux/files/home/nltk_data/#corpora/shakespeare/shakespeare.txt"#----#"/data/data/com.termux/files/home/nltk_data/#corpora/shakespeare/shakespeare-caesar.txt"#----#"/data/data/com.termux/files/home/nltk_data/#corpora/#genesis/english-kjv.txt"#----#"/data/data/com.termux/files/home/ai_assistant/#webscr_nltk/wordgen/shakeparsed.txt"#-----#"/data/data/com.termux/files/home/nltk_data/#corpora/shakespeare/shakespeare.txt"#------#"/data/data/com.termux/files/home/nltk_data/#corpora/webtext/overheard.txt"#-----#"/data/data/com.termux/files/home/nltk_data/#corpora/genesis/english-kjv.txt"#----#["./training_data/fellowship_of_the_ring.txt", "./#training_data/two_towers.txt", "./training_data/#return_of_the_king.txt"]#....................................................................def main():    if valid_args(sys.argv):        if not existing_model():            model = load_data(TRAINING_DATA)            length = int(sys.argv[2])            result = OPTIONS[sys.argv[1]](length, model)            print(result)      #                 print(model)            return True        else:            model = load_model()            length = int(sys.argv[2])            result = OPTIONS[sys.argv[1]](length, model)            print(result)    #                   print(model)            return True    else:        print_usage()def is_positive_int(str):    try:        length = int(str)        if length > 0:            return True        else:            return False            print("Must provide a positive integer!")    except:        return Falsedef existing_model():    return os.path.isfile(MODEL_NAME)    def save_model(markov_model):    with open('model.pkl', 'wb') as f:        pickle.dump(markov_model, f, pickle.HIGHEST_PROTOCOL)    with open('model.json', 'w') as f:        json.dump(markov_model, f, indent=4)    return True    def load_model():    with open('model.pkl', 'rb') as f:#        model=json.load(f)        model = pickle.load(f)#    print(model)      #print('\n') #"Model loaded!")        return modeldef valid_args(args):   #FILE_NAME <sentence(s)> <int>    if len(args) == 3:        if args[1] == "sentence" or args[1] == "sentences":            if is_positive_int(args[2]):                return True    return Falsedef print_usage():    print("Usage:")    print("sentence <int>  | sentence of <int> length")    print("sentences <int> | <int> # of sentences")def load_data(file_path):    """ Open and format data from file_path(s), returns a trained markov model """    if isinstance(file_path, list):        for path in file_path:            corpus = []            raw_data = get_data(path)            corpus += split_into_sentences(raw_data)            markov_model = make_markov_model(corpus)    else:        raw_data = get_data(file_path)        corpus = split_into_sentences(raw_data)        markov_model = make_markov_model(corpus)    return markov_model         ## here is where we can save the modeldef make_markov_model(corpus):    markov_model = dict()    corpus_size = 0    for sentence in corpus:        if "START" in markov_model:          #START            markov_model["START"].update(sentence[0])        else:            markov_model["START"] = FrequencyGram(sentence[0])        if sentence[-1] in markov_model: # last word/char always points to "END"            markov_model[sentence[-1]].update("END")                                                         #[-1]]              ("END")          else:            markov_model[sentence[-1]] = FrequencyGram("END")        corpus_size += len(sentence)        for i in range(0, len(sentence)-1): # (sentence)-1):a ## skipping last word because we already added it in #prev block            if sentence[i] in markov_model: markov_model[sentence[i]].update(sentence[i+1])            else:                markov_model[sentence[i]] = FrequencyGram(sentence[i+1])    markov_model["END"] = FrequencyGram("START") #                              ["END"] ^                                 ("START")^### Since we're adding in START/END has we iterate #instead of having it in the raw  data,                                                 ## we have to manually add a FrequencyGram at key #END to ensure our sentences flow    print("Corpus: {0} words".format(corpus_size))    if save_model(markov_model):        input("enter any key to exit (save_model)")        exit()#print('\n')#"Model saved!")    else:        print("An error occurred saving your model")    return markov_modeldef generate_random_start(markov_model):    sentence_starter = markov_model["START"].return_weighted_rand_word()    return sentence_starter    def generate_n_length_sentence(length, markov_model):    sentence=["" for x in range(length)]    start_word = generate_random_start(markov_model)  #  sentence+=[start_word]    for i in range(0, len(sentence[::4])-1):                     if start_word =="START" or start_word =="END":continue        elif start_word not in PUNCS:#            sentence[i]+= start_word+' '             #print('B\n\n', sentence)            current_frequencygram=markov_model[start_word]            #print('C\n', i, '\n', current_frequencygram)            current_word=current_frequencygram.return_weighted_rand_word()            #print('C1\n\n', current_word)        if current_word =="START" or current_word =="END":continue        elif current_word not in PUNCS:            sentence[i]+= current_word+' '             current_frequencygram=markov_model[current_word]            #print('E\n\n', current_frequencygram)                         next_word=current_frequencygram.return_weighted_rand_word()            #print('E1\n\n', next_word)        if next_word =="START" or current_word =="END":continue        elif next_word not in PUNCS:            current_word=next_word            sentence[i]+= current_word+' '             current_frequencygram=markov_model[next_word]            #print('F\n\n', current_frequencygram)            next_after=current_frequencygram.return_weighted_rand_word()            #print('F1\n\n', next_after)        if next_after =="START" or next_after =="END":continue        elif next_after not in PUNCS:            current_word=next_after            sentence[i]+= current_word+' '             current_frequencygram=markov_model[next_after]            #print('G\n\n', current_frequencygram)            word_after=current_frequencygram.return_weighted_rand_word()            #print('G1\n\n', word_after)        if word_after =="START" or word_after =="END":continue        elif word_after not in PUNCS:            current_word=word_after            sentence[i]+= current_word+' '            current_frequencygram=markov_model[word_after]            #print('H\n\n', current_frequencygram)            current_after=current_frequencygram.return_weighted_rand_word()            #print('H1\n\n', current_after)        if current_after =="START" or current_after =="END":continue        elif current_after not in PUNCS:            current_word=current_after            sentence[i]+= current_word+' '             current_frequencygram=markov_model[current_after]            #print('I\n\n', current_frequencygram)            next2_last = current_frequencygram.return_weighted_rand_word()             #print('I1\n\n', next2_last)        if next2_last =="START" or next2_last =="END":continue        elif next2_last not in PUNCS:            current_word=next2_last            sentence[i]+= current_word+' '            current_frequencygram=markov_model[next2_last]            last_word = current_frequencygram.return_weighted_rand_word()            #print('J\n\n', current_frequencygram)                  #print('J1\n\n', last_word)        if last_word =="START" or last_word =="END":continue        elif last_word not in PUNCS:            current_word=last_word            sentence[i]+= current_word+' ' #            current_word, next_word, next_after, word_after, current_after=next_word, next_after, word_after, current_after,current_word              #          sentence[i]+= '1'+current_word+'1 2'+next_word+'2 #3'+next_after+'3 4'+word_after+'4 5'+current_after+'5# 6'+last_word+'6 1'       #     current_word=last_word#            sentence[i]+=current_word            #                #print('\n\n\n', i, "".join(sentence), '\n\n\n\n') #       elif i==0 and next_word in PUNCS or next_word not in PUNCS: #           current_word=next_word #           sentence[i]+=current_word#                print("  ", i, sentence, '\n')#        else:   #         sentence[i]+= current_word#                print("".join(sentence))    print("".join(sentence))           #    return format_text(sentence)def format_text(sentence):    formatted =""     for i in range(0, len(sentence)):        if sentence[i] not in PUNCS:            formatted += sentence[i] + " "        elif sentence[i] in PUNCS and i == 0:            formatted==formatted+ " "        else:            formatted = formatted[:-1]    print("".join(formatted))def generate_n_sentences(length, markov_model):    sentences = ["" for x in range(length)]    current_word = generate_random_start(markov_model)    current_frequencygram = markov_model[current_word]    for i in range(0, length):                next_word = current_frequencygram.return_weighted_rand_word()        if next_word == "END":continue        current_word = next_word        if current_word not in PUNCS:            sentences[i] += " " + current_word    #                print(sentences, "    ", i, '\n')        else:            sentences[i] += current_word    print(" ".join(sentences))    def get_data(path):    with open(path) as file:        data = file.read()    file.close()    return datadef generate_s_start(markov_model):        sentence_starter = markov_model["START"].return_weighted_rand_word()    return sentence_starterdef generate_s_length_sentence(length, markov_model):    ws_limit=11    sc=0    current_s_word = generate_s_start(markov_model)    sentence = ["" for x in range(length)]    current_s_frequencygram = markov_model[current_s_word]    for i in range(sc, ws_limit):        next_s_word = current_s_frequencygram.return_weighted_rand_word()#        current_s_word=next_s_word#        sc+=s(current_s_word)                           if current_s_word == "START" or current_s_word =="END":break        else:            current_s_word=next_s_word            sc+=s(current_s_word)                                sentence+=current_s_word+' '    return ''.join(sentence) #format_s_text(sentence)def format_s_text(ssentence_list):    sformatted = ""    for i in range(0, len(ssentence_list)-1):        if ssentence_list[i] not in PUNCS:            sformatted += ssentence_list + ""        elif ssentence_list[i] in PUNCS and i == 0:            sformatted = ssentence_list        else:            sformatted = sformatted[:-1] + ssentence_list    return ''.join(sformatted)def generate_s_sentences(length, markov_model):    from os import system as oS    sentences = [""]#    #print('\n\nenter a number\n\n')#    slength=input()    current_sentence=generate_n_length_sentence(length, markov_model)#oS('python3 bctg.py sentence 8')    for i in range(length-1):        gen_sentence=generate_n_length_sentence(length, markov_model)#oS('python3 bctg.py sentence 9')        next_sentence=generate_n_length_sentence(length, markov_model)#oS('python3 bctg.py sentence 10')        current_sentence, next_sentence, gen_sentence=next_sentence, gen_sentence, current_sentence        sentences[i] += current_sentence# + gen_sentence + next_sentence        break    print(sentences)        # D Greenburg // split_into_sentences    #https://stackoverflow.com/questions/4576077/python#-split-text-on-sentences        def split_into_sentences(text):    text = " " + text + " "    text = text.replace("\n","  ").replace("\r", " ")    text = re.sub(' +',' ',text)    text = re.sub(PREFIXES,"\\1<prd>",text)    text = re.sub(WEBSITES,"<prd>\\1",text)    if "Ph.D" in text: text = text.replace("Ph.D.","Ph<prd>D<prd>")#    print(text)    #input("enter any key to continue")  #  os.system("clear")    text = re.sub("\s" + ALPHABETS + "[.] "," \\1<prd>\\n ",text)    text = re.sub(ACRONYMS+" "+STARTERS,"\\1<stop> \\2",text)    text = re.sub(ALPHABETS + "[.] " + ALPHABETS + "[.] " + ALPHABETS + "[.] ","\\1<prd>\\2<prd>\\3<prd>",text)    text = re.sub(ALPHABETS + "[.] " + ALPHABETS + "[.] ","\\1<prd>\\2<prd>",text)#    text = re.sub(" "+SUFFIXES+"[.] "+STARTERS," \#\1<stop> \\2",text)                       #^= .    text = re.sub(" "+SUFFIXES+"[.] "+STARTERS," \\1<stop> \\2",text)    text = re.sub(" "+SUFFIXES+"[.] "," \\1<prd>",text)#                                                        ^= .    text = re.sub(" " + ALPHABETS + "[.] "," \\1<prd>",text) #   print(text)#    input("enter any key to continue")#    os.system("clear")    if "'" in text: text = text.replace("'\'","\''")    if "”" in text: text = text.replace(".”","”.")    if "\"" in text: text = text.replace(".\"","\".")    if "!" in text: text = text.replace("!\"","\"!")    if "?" in text: text = text.replace("?\"","\"?")    if "," in text: text = text.replace(",\"","\",")#added*    if ";" in text: text = text.replace(";\"","\";")#    if ":" in text: text = text.replace(":\"","\":")# #   print(text)#    input("enter any key to continue")#    os.system("clear")    text = text.replace(",",",<stop>")#    text = text.replace(";",";<stop>")#    text = text.replace(":",":<stop>")#    text = text.replace(".",".<stop>")    text = text.replace("?","?<stop>")    text = text.replace("!","!<stop>")    text = text.replace("<prd>",".")     sentences = text.split("<stop>")    sentences = sentences[:-1]    sentences = [s.strip() for s in sentences]#     Added line to split sentence lists into lists of words/#punctuation#    sentences = [ re.findall(r"[\w]+|[^\s\w]", sentence) for        sentence in text ]            for i in range(0, len(sentences)):        sentences[i] = re.findall(r"[\w']+|[.,!?;]", sentences[i])            return sentencesOPTIONS = { "sentence": generate_n_length_sentence,                "sentences": generate_s_sentences }       if __name__ == "__main__":    main()    #print('\n ')    